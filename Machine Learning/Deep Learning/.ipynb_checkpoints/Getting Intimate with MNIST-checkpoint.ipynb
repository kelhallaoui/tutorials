{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Intimate with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database found [here](http://yann.lecun.com/exdb/mnist/) from Yann LeCun et friends. \n",
    "\n",
    "It is a collection of 70,000 handwritten numbers as greyscale images of size $28$ by $28$. This database is by far the most popular in machine learning publications, it is a good means by which to benchmark an algorithm or model architecture. Furthermore, the simplicity of the problem allows for reasonable training times whilst still achieving impressive results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the MNIST data\n",
    "\n",
    "Luckily the MNIST data is available through the keras framework. This is very convenient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (60000, 28, 28)\n",
      "Testing data shape :  (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "\n",
    "print('Training data shape: ', x_train.shape)\n",
    "print('Testing data shape : ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAEWCAYAAABG/79mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e2c39557b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "training_counts = [None] * 10 \n",
    "testing_counts = [None] * 10\n",
    "for i in range(10):\n",
    "    training_counts[i] = len(y_train[y_train == i])/len(y_train)\n",
    "    testing_counts[i] = len(y_test[y_test == i])/len(y_test)\n",
    "\n",
    "# the histogram of the data\n",
    "train_bar = plt.bar(np.arange(10)-0.2, training_counts, align='center', color = 'r', alpha=0.75, width = 0.41, label='Training')\n",
    "test_bar = plt.bar(np.arange(10)+0.2, testing_counts, align='center', color = 'b', alpha=0.75, width = 0.41, label = 'Testing')\n",
    "\n",
    "plt.xlabel('Labels')\n",
    "plt.xticks((0,1,2,3,4,5,6,7,8,9))\n",
    "plt.ylabel('Count (%)')\n",
    "plt.title('Label distribution in the training and test set')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), handles=[train_bar, test_bar], loc=2)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing MNIST images to the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# utility function for showing images\n",
    "def show_imgs(x_test, decoded_imgs=None, n=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        if decoded_imgs is not None:\n",
    "            ax = plt.subplot(2, n, i+ 1 +n)\n",
    "            plt.imshow(decoded_imgs[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAAAAeMD/A63YWa1CBmm0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e2c66f72e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels:  [5 0 4 1 9 2 1 3 1 4]\n",
      "Testing labels :  [7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "show_imgs(x_train, x_test)\n",
    "print('Training labels: ', y_train[0:10])\n",
    "print('Testing labels : ', y_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the computer to be able to read these images and determine what number is written within its boundary. To do this we will use machine leanrning for the purpose of classification. The demonstrated success of neural network algorithms on this particular problem was the beginning of the pursuit for deep learning. Since that start, the underlying reasons as to why neural networks are particularly well suited for interpreting images has been demonstrated and extended to solve many more complex problems. \n",
    "\n",
    "The \n",
    "\n",
    "Generally, this means going from a feature-set, the MNIST images, and mapping that input to an output, the value of the number. \n",
    "\n",
    "Classification pertains to going from a set of features and determining which is the most likely "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data to be used in the model. We need to reshape the inputs and outputs such that their channel is their last dimension.\n",
    "\n",
    "Furthermore, the outputs are converted to 1-hot encoding. This means that a vector index coinciding with the class is set high to indicate a certain label.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "$0 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]$ <br>\n",
    "$1 = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]$ <br>\n",
    "$...$ <br>\n",
    "$9 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]$ <br>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The known number of output classes.\n",
    "num_classes = 10\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Channels go last for TensorFlow backend\n",
    "x_train_reshaped = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Convert class vectors to binary class matrices. This uses 1 hot encoding.\n",
    "y_train_binary = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a sequential model in Keras describing your architecture.\n",
    "\n",
    "We are using:\n",
    "- **Convolution:** 32, 3x3 filters with rectifier linear unit\n",
    "- **Convolution:** 64, 3x3 filters with rectifier linear unit\n",
    "- **Max Pooling:** 2x2 pool size\n",
    "- **Dropout:** drop 25% of the tensor\n",
    "- **Dense:** 128 fully connected nodes using rectifier linear unit\n",
    "- **Dropout:** drop 50% of the tensor\n",
    "- **Dense:** output has 10 nodes using softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model, and set up a callback such that at every epoch the weights are saved to the disk. Then we fit the weights to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.9002Epoch 00001: val_acc improved from -inf to 0.97340, saving model to weights/weights-improvement-01-0.97.hdf5\n",
      "60000/60000 [==============================] - 162s 3ms/step - loss: 0.3307 - acc: 0.9003 - val_loss: 0.0794 - val_acc: 0.9734\n",
      "Epoch 2/4\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9664Epoch 00002: val_acc improved from 0.97340 to 0.98170, saving model to weights/weights-improvement-02-0.98.hdf5\n",
      "60000/60000 [==============================] - 164s 3ms/step - loss: 0.1140 - acc: 0.9664 - val_loss: 0.0557 - val_acc: 0.9817\n",
      "Epoch 3/4\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9750Epoch 00003: val_acc improved from 0.98170 to 0.98550, saving model to weights/weights-improvement-03-0.99.hdf5\n",
      "60000/60000 [==============================] - 169s 3ms/step - loss: 0.0853 - acc: 0.9749 - val_loss: 0.0438 - val_acc: 0.9855\n",
      "Epoch 4/4\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9782Epoch 00004: val_acc improved from 0.98550 to 0.98730, saving model to weights/weights-improvement-04-0.99.hdf5\n",
      "60000/60000 [==============================] - 171s 3ms/step - loss: 0.0728 - acc: 0.9782 - val_loss: 0.0380 - val_acc: 0.9873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2c5c10c88>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model_json = model.to_json()\n",
    "with open(\"weights/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save the weights using a checkpoint.\n",
    "filepath=\"weights/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "epochs = 4\n",
    "batch_size = 128\n",
    "# Fit the model weights.\n",
    "model.fit(x_train_reshaped, y_train_binary,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(x_test_reshaped, y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy:\n",
      "Test loss: 0.037953859773022125\n",
      "Test accuracy: 0.9873\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_reshaped, y_test_binary, verbose=0)\n",
    "print('Model accuracy:')\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict the classes: \n",
      "\r",
      "10/10 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAABzCAYAAAAfb55ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e2c66eea20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes:  [0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "print('Predict the classes: ')\n",
    "prediction = model.predict_classes(x_test_reshaped[10:20])\n",
    "show_imgs(x_test[10:20])\n",
    "print('Predicted classes: ', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAABzCAYAAAAfb55ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e2c9548fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes:  [7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "# load model saved as a json and create model\n",
    "json_file = open('weights/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"weights/weights-improvement-04-0.99.hdf5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "#loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "predictions = loaded_model.predict_classes(x_test_reshaped[0:10], verbose=0)\n",
    "show_imgs(x_test[0:10])\n",
    "print('Predicted classes: ', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'class_name': 'Conv2D',\n",
       "  'config': {'activation': 'relu',\n",
       "   'activity_regularizer': None,\n",
       "   'batch_input_shape': (None, 28, 28, 1),\n",
       "   'bias_constraint': None,\n",
       "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "   'bias_regularizer': None,\n",
       "   'data_format': 'channels_last',\n",
       "   'dilation_rate': (1, 1),\n",
       "   'dtype': 'float32',\n",
       "   'filters': 32,\n",
       "   'kernel_constraint': None,\n",
       "   'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "    'config': {'distribution': 'uniform',\n",
       "     'mode': 'fan_avg',\n",
       "     'scale': 1.0,\n",
       "     'seed': None}},\n",
       "   'kernel_regularizer': None,\n",
       "   'kernel_size': (3, 3),\n",
       "   'name': 'conv2d_3',\n",
       "   'padding': 'valid',\n",
       "   'strides': (1, 1),\n",
       "   'trainable': True,\n",
       "   'use_bias': True}},\n",
       " {'class_name': 'Conv2D',\n",
       "  'config': {'activation': 'relu',\n",
       "   'activity_regularizer': None,\n",
       "   'bias_constraint': None,\n",
       "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "   'bias_regularizer': None,\n",
       "   'data_format': 'channels_last',\n",
       "   'dilation_rate': (1, 1),\n",
       "   'filters': 64,\n",
       "   'kernel_constraint': None,\n",
       "   'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "    'config': {'distribution': 'uniform',\n",
       "     'mode': 'fan_avg',\n",
       "     'scale': 1.0,\n",
       "     'seed': None}},\n",
       "   'kernel_regularizer': None,\n",
       "   'kernel_size': (3, 3),\n",
       "   'name': 'conv2d_4',\n",
       "   'padding': 'valid',\n",
       "   'strides': (1, 1),\n",
       "   'trainable': True,\n",
       "   'use_bias': True}},\n",
       " {'class_name': 'MaxPooling2D',\n",
       "  'config': {'data_format': 'channels_last',\n",
       "   'name': 'max_pooling2d_2',\n",
       "   'padding': 'valid',\n",
       "   'pool_size': (2, 2),\n",
       "   'strides': (2, 2),\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dropout',\n",
       "  'config': {'name': 'dropout_3',\n",
       "   'noise_shape': None,\n",
       "   'rate': 0.25,\n",
       "   'seed': None,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Flatten', 'config': {'name': 'flatten_2', 'trainable': True}},\n",
       " {'class_name': 'Dense',\n",
       "  'config': {'activation': 'relu',\n",
       "   'activity_regularizer': None,\n",
       "   'bias_constraint': None,\n",
       "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "   'bias_regularizer': None,\n",
       "   'kernel_constraint': None,\n",
       "   'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "    'config': {'distribution': 'uniform',\n",
       "     'mode': 'fan_avg',\n",
       "     'scale': 1.0,\n",
       "     'seed': None}},\n",
       "   'kernel_regularizer': None,\n",
       "   'name': 'dense_3',\n",
       "   'trainable': True,\n",
       "   'units': 128,\n",
       "   'use_bias': True}},\n",
       " {'class_name': 'Dropout',\n",
       "  'config': {'name': 'dropout_4',\n",
       "   'noise_shape': None,\n",
       "   'rate': 0.5,\n",
       "   'seed': None,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dense',\n",
       "  'config': {'activation': 'softmax',\n",
       "   'activity_regularizer': None,\n",
       "   'bias_constraint': None,\n",
       "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "   'bias_regularizer': None,\n",
       "   'kernel_constraint': None,\n",
       "   'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "    'config': {'distribution': 'uniform',\n",
       "     'mode': 'fan_avg',\n",
       "     'scale': 1.0,\n",
       "     'seed': None}},\n",
       "   'kernel_regularizer': None,\n",
       "   'name': 'dense_4',\n",
       "   'trainable': True,\n",
       "   'units': 10,\n",
       "   'use_bias': True}}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.summary()\n",
    "\n",
    "loaded_model.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a minimal feature set from which images can be recovered\n",
    "\n",
    "An autoencoder can be used to extract features, we will build a symmetric network where the center is a constrained set of neurons. We will then train the network by feeding it the MNIST images and tuning the model's parameters to obtain a reconstruction of the original image at the output. If this results in a sufficiently good reconstruction, we can then determine that the features extracted by the constrained hidden layer can adequately represent the MNIST image. \n",
    "\n",
    "The information entropy in the constrained set is much higher than the input space, thus these features can be used for small data classification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Flatten()(x) #Test\n",
    "encoded = Dense(128)(x) # Test\n",
    "#encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Dense(128)(encoded)\n",
    "x = Reshape((4,4,8))(x)\n",
    "#x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 8)           584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 37,409\n",
      "Trainable params: 37,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " 8576/60000 [===>..........................] - ETA: 1:26 - loss: 0.3664"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-2c1e17563b4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m autoencoder.fit(x_train_reshaped, x_train_reshaped, epochs=epochs, batch_size=batch_size,\n\u001b[0;32m      6\u001b[0m                \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_reshaped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_reshaped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "autoencoder.fit(x_train_reshaped, x_train_reshaped, epochs=epochs, batch_size=batch_size,\n",
    "               shuffle=True, validation_data=(x_test_reshaped, x_test_reshaped), verbose=1,\n",
    "               callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (upper row)\n",
      "decoded (bottom row)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x260aa615240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test_reshaped)\n",
    "print(\"input (upper row)\\ndecoded (bottom row)\")\n",
    "show_imgs(x_test_reshaped, decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising autoencoder\n",
    "\n",
    "The autoencoder technique can lead to trivial solutions in the constrained layer; it may learn the identity transformation where $X' = XI$. Thus, it is proposed to use a noisy version of the images to train the autoencoder such that the differences in the distributions across the images are in fact retained as a compressed set of features needed to reconstruct the clean image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAABzCAYAAAAfb55ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x260b20e9ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add random noise before training!\n",
    "noise_factor = 0.5 \n",
    "x_train_noisy = x_train_reshaped + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train_reshaped.shape) \n",
    "x_test_noisy = x_test_reshaped + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test_reshaped.shape) \n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.) \n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "show_imgs(x_test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 7, 7, 8)           584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 8, 8, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 37,409\n",
      "Trainable params: 37,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Flatten()(x) #Test\n",
    "encoded = Dense(128)(x) # Test\n",
    "#encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Dense(128)(encoded)\n",
    "x = Reshape((4,4,8))(x)\n",
    "#x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 171s 3ms/step - loss: 0.2755 - val_loss: 0.2114\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 0.1942 - val_loss: 0.1761\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 0.1724 - val_loss: 0.1645\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 160s 3ms/step - loss: 0.1608 - val_loss: 0.1542\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 168s 3ms/step - loss: 0.1531 - val_loss: 0.1489\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 166s 3ms/step - loss: 0.1481 - val_loss: 0.1479\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 164s 3ms/step - loss: 0.1443 - val_loss: 0.1430\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 171s 3ms/step - loss: 0.1412 - val_loss: 0.1387\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 166s 3ms/step - loss: 0.1386 - val_loss: 0.1386\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 161s 3ms/step - loss: 0.1368 - val_loss: 0.1338\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 152s 3ms/step - loss: 0.1349 - val_loss: 0.1317\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 154s 3ms/step - loss: 0.1337 - val_loss: 0.1309\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 162s 3ms/step - loss: 0.1323 - val_loss: 0.1311\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 164s 3ms/step - loss: 0.1311 - val_loss: 0.1326\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 166s 3ms/step - loss: 0.1302 - val_loss: 0.1273\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 162s 3ms/step - loss: 0.1291 - val_loss: 0.1311\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 164s 3ms/step - loss: 0.1284 - val_loss: 0.1266\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 168s 3ms/step - loss: 0.1276 - val_loss: 0.1260\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 165s 3ms/step - loss: 0.1270 - val_loss: 0.1263\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 0.1263 - val_loss: 0.1242\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 166s 3ms/step - loss: 0.1257 - val_loss: 0.1235\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 162s 3ms/step - loss: 0.1252 - val_loss: 0.1233\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 161s 3ms/step - loss: 0.1248 - val_loss: 0.1259\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 161s 3ms/step - loss: 0.1239 - val_loss: 0.1248\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 161s 3ms/step - loss: 0.1234 - val_loss: 0.1207\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 161s 3ms/step - loss: 0.1232 - val_loss: 0.1244\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 110s 2ms/step - loss: 0.1226 - val_loss: 0.1232\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.1221 - val_loss: 0.1206\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.1219 - val_loss: 0.1223\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.1216 - val_loss: 0.1199\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1210 - val_loss: 0.1206\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1208 - val_loss: 0.1227\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1205 - val_loss: 0.1193\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1201 - val_loss: 0.1189\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1197 - val_loss: 0.1183\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1195 - val_loss: 0.1210\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.1192 - val_loss: 0.1177\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1189 - val_loss: 0.1177\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1187 - val_loss: 0.1178\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1185 - val_loss: 0.1179\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1183 - val_loss: 0.1169\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1183 - val_loss: 0.1173\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1177 - val_loss: 0.1195\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1175 - val_loss: 0.1157\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.1174 - val_loss: 0.1193\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.1172 - val_loss: 0.1161\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1168 - val_loss: 0.1172\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.1169 - val_loss: 0.1160\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.1166 - val_loss: 0.1152\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1164 - val_loss: 0.1171\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1162 - val_loss: 0.1154\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.1159 - val_loss: 0.1148\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1157 - val_loss: 0.1186\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 108s 2ms/step - loss: 0.1159 - val_loss: 0.1159\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1158 - val_loss: 0.1137\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1153 - val_loss: 0.1151\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1153 - val_loss: 0.1144\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1151 - val_loss: 0.1146\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1152 - val_loss: 0.1127\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1150 - val_loss: 0.1155\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1150 - val_loss: 0.1134\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1148 - val_loss: 0.1133\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.1146 - val_loss: 0.1133\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1144 - val_loss: 0.1132\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1144 - val_loss: 0.1122\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1141 - val_loss: 0.1131\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1140 - val_loss: 0.1123\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1139 - val_loss: 0.1127\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1139 - val_loss: 0.1149\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1136 - val_loss: 0.1124\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.1136 - val_loss: 0.1130\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.1135 - val_loss: 0.1117\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1134 - val_loss: 0.1126\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1134 - val_loss: 0.1147\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1134 - val_loss: 0.1118\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1134 - val_loss: 0.1123\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1130 - val_loss: 0.1118\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1132 - val_loss: 0.1129\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1130 - val_loss: 0.1137\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 108s 2ms/step - loss: 0.1129 - val_loss: 0.1127\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1128 - val_loss: 0.1112\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1126 - val_loss: 0.1110\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1125 - val_loss: 0.1115\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1127 - val_loss: 0.1112\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1127 - val_loss: 0.1119\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1125 - val_loss: 0.1137\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1123 - val_loss: 0.1113\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1122 - val_loss: 0.1111\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.1122 - val_loss: 0.1109\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1122 - val_loss: 0.1109\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1122 - val_loss: 0.1112\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1122 - val_loss: 0.1114\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1120 - val_loss: 0.1119\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1120 - val_loss: 0.1103\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.1119 - val_loss: 0.1107\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1117 - val_loss: 0.1107\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.1117 - val_loss: 0.1118\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1116 - val_loss: 0.1104\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1116 - val_loss: 0.1115\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.1115 - val_loss: 0.1102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x260c073db00>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# it takes more epochs to converge\n",
    "autoencoder.fit(x_train_noisy, x_train_reshaped, epochs=epochs, batch_size=batch_size,\n",
    "                shuffle=True, validation_data=(x_test_noisy, x_test_reshaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe how the training loss and the validation loss changed throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'validation loss')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAEICAYAAAATE/N5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x260c07b7dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training history\")\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(autoencoder.history.history['loss'])\n",
    "ax1.set_title('loss')\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "plt.plot(autoencoder.history.history['val_loss'])\n",
    "ax2.set_title('validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting mnist\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting mnist\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting mnist\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting mnist\\t10k-labels-idx1-ubyte.gz\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_26 (Conv2D)           (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.692405, acc: 0.533203]  [A loss: 1.438110, acc: 0.000000]\n",
      "1: [D loss: 0.626756, acc: 0.990234]  [A loss: 2.725226, acc: 0.000000]\n",
      "2: [D loss: 0.490814, acc: 0.878906]  [A loss: 1.330547, acc: 0.000000]\n",
      "3: [D loss: 0.623972, acc: 0.505859]  [A loss: 8.279166, acc: 0.000000]\n",
      "4: [D loss: 0.364121, acc: 0.820312]  [A loss: 0.082273, acc: 1.000000]\n",
      "5: [D loss: 0.714951, acc: 0.505859]  [A loss: 4.629430, acc: 0.000000]\n",
      "6: [D loss: 0.110761, acc: 0.964844]  [A loss: 0.026552, acc: 1.000000]\n",
      "7: [D loss: 0.101266, acc: 0.992188]  [A loss: 0.055241, acc: 1.000000]\n",
      "8: [D loss: 0.099494, acc: 0.988281]  [A loss: 0.017710, acc: 1.000000]\n",
      "9: [D loss: 0.125682, acc: 0.982422]  [A loss: 0.013025, acc: 1.000000]\n",
      "10: [D loss: 0.113917, acc: 0.994141]  [A loss: 0.010693, acc: 1.000000]\n",
      "11: [D loss: 0.124292, acc: 0.988281]  [A loss: 0.008671, acc: 1.000000]\n",
      "12: [D loss: 0.125487, acc: 0.990234]  [A loss: 0.004707, acc: 1.000000]\n",
      "13: [D loss: 0.126816, acc: 0.994141]  [A loss: 0.003815, acc: 1.000000]\n",
      "14: [D loss: 0.116026, acc: 0.998047]  [A loss: 0.003676, acc: 1.000000]\n",
      "15: [D loss: 0.120246, acc: 0.994141]  [A loss: 0.002535, acc: 1.000000]\n",
      "16: [D loss: 0.149239, acc: 0.982422]  [A loss: 0.000370, acc: 1.000000]\n",
      "17: [D loss: 0.111472, acc: 1.000000]  [A loss: 0.000533, acc: 1.000000]\n",
      "18: [D loss: 0.110593, acc: 0.994141]  [A loss: 0.000441, acc: 1.000000]\n",
      "19: [D loss: 0.098477, acc: 0.998047]  [A loss: 0.000392, acc: 1.000000]\n",
      "20: [D loss: 0.083148, acc: 1.000000]  [A loss: 0.000987, acc: 1.000000]\n",
      "21: [D loss: 0.087207, acc: 0.994141]  [A loss: 0.000140, acc: 1.000000]\n",
      "22: [D loss: 0.071887, acc: 0.998047]  [A loss: 0.000322, acc: 1.000000]\n",
      "23: [D loss: 0.069468, acc: 0.996094]  [A loss: 0.000277, acc: 1.000000]\n",
      "24: [D loss: 0.054593, acc: 1.000000]  [A loss: 0.000155, acc: 1.000000]\n",
      "25: [D loss: 0.059530, acc: 0.996094]  [A loss: 0.000053, acc: 1.000000]\n",
      "26: [D loss: 0.051223, acc: 0.996094]  [A loss: 0.000020, acc: 1.000000]\n",
      "27: [D loss: 0.038609, acc: 1.000000]  [A loss: 0.000045, acc: 1.000000]\n",
      "28: [D loss: 0.033117, acc: 0.998047]  [A loss: 0.000085, acc: 1.000000]\n",
      "29: [D loss: 0.026200, acc: 0.998047]  [A loss: 0.000071, acc: 1.000000]\n",
      "30: [D loss: 0.025603, acc: 0.996094]  [A loss: 0.000037, acc: 1.000000]\n",
      "31: [D loss: 0.040280, acc: 0.992188]  [A loss: 0.000001, acc: 1.000000]\n",
      "32: [D loss: 0.030816, acc: 1.000000]  [A loss: 0.000002, acc: 1.000000]\n",
      "33: [D loss: 0.020810, acc: 1.000000]  [A loss: 0.000028, acc: 1.000000]\n",
      "34: [D loss: 0.018883, acc: 0.998047]  [A loss: 0.000004, acc: 1.000000]\n",
      "35: [D loss: 0.022380, acc: 0.998047]  [A loss: 0.000005, acc: 1.000000]\n",
      "36: [D loss: 0.015404, acc: 1.000000]  [A loss: 0.000081, acc: 1.000000]\n",
      "37: [D loss: 0.020663, acc: 0.998047]  [A loss: 0.000023, acc: 1.000000]\n",
      "38: [D loss: 0.030018, acc: 0.992188]  [A loss: 0.000000, acc: 1.000000]\n",
      "39: [D loss: 0.048022, acc: 0.992188]  [A loss: 0.016404, acc: 1.000000]\n",
      "40: [D loss: 0.083974, acc: 0.980469]  [A loss: 13.586605, acc: 0.000000]\n",
      "41: [D loss: 4.640562, acc: 0.501953]  [A loss: 16.118101, acc: 0.000000]\n",
      "42: [D loss: 8.056562, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "43: [D loss: 8.052380, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "44: [D loss: 8.049437, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "45: [D loss: 8.039577, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "46: [D loss: 7.964580, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "47: [D loss: 6.777664, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "48: [D loss: 6.705306, acc: 0.496094]  [A loss: 4.257970, acc: 0.292969]\n",
      "49: [D loss: 6.735998, acc: 0.501953]  [A loss: 0.045602, acc: 0.980469]\n",
      "50: [D loss: 3.238479, acc: 0.478516]  [A loss: 16.118101, acc: 0.000000]\n",
      "51: [D loss: 7.906975, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "52: [D loss: 7.057125, acc: 0.500000]  [A loss: 0.901655, acc: 0.800781]\n",
      "53: [D loss: 5.925916, acc: 0.490234]  [A loss: 16.080870, acc: 0.000000]\n",
      "54: [D loss: 5.419191, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "55: [D loss: 7.290915, acc: 0.480469]  [A loss: 0.686090, acc: 0.800781]\n",
      "56: [D loss: 6.688037, acc: 0.468750]  [A loss: 14.963554, acc: 0.000000]\n",
      "57: [D loss: 3.838836, acc: 0.380859]  [A loss: 16.118101, acc: 0.000000]\n",
      "58: [D loss: 7.969250, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "59: [D loss: 7.949304, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "60: [D loss: 7.681518, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "61: [D loss: 6.545775, acc: 0.500000]  [A loss: 1.747657, acc: 0.609375]\n",
      "62: [D loss: 8.010992, acc: 0.486328]  [A loss: 0.000000, acc: 1.000000]\n",
      "63: [D loss: 8.026381, acc: 0.492188]  [A loss: 0.000000, acc: 1.000000]\n",
      "64: [D loss: 7.983413, acc: 0.492188]  [A loss: 0.000000, acc: 1.000000]\n",
      "65: [D loss: 7.994936, acc: 0.494141]  [A loss: 0.000000, acc: 1.000000]\n",
      "66: [D loss: 7.985488, acc: 0.494141]  [A loss: 0.000000, acc: 1.000000]\n",
      "67: [D loss: 7.988302, acc: 0.496094]  [A loss: 0.000000, acc: 1.000000]\n",
      "68: [D loss: 7.985120, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "69: [D loss: 8.007368, acc: 0.484375]  [A loss: 0.000000, acc: 1.000000]\n",
      "70: [D loss: 7.989674, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "71: [D loss: 7.982431, acc: 0.496094]  [A loss: 0.000000, acc: 1.000000]\n",
      "72: [D loss: 7.993645, acc: 0.492188]  [A loss: 0.000000, acc: 1.000000]\n",
      "73: [D loss: 8.006093, acc: 0.492188]  [A loss: 0.000000, acc: 1.000000]\n",
      "74: [D loss: 7.972095, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "75: [D loss: 7.977525, acc: 0.496094]  [A loss: 0.000000, acc: 1.000000]\n",
      "76: [D loss: 7.981821, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "77: [D loss: 7.973003, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "78: [D loss: 7.975848, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "79: [D loss: 7.986024, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "80: [D loss: 7.971500, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "81: [D loss: 7.973266, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "82: [D loss: 7.985629, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "83: [D loss: 7.972158, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "84: [D loss: 7.974109, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "85: [D loss: 7.980508, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "86: [D loss: 7.972703, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "87: [D loss: 7.972131, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "88: [D loss: 7.972238, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "89: [D loss: 7.978218, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "90: [D loss: 7.971521, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "91: [D loss: 7.971261, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "92: [D loss: 7.972568, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "93: [D loss: 7.971239, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "94: [D loss: 7.971359, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "95: [D loss: 7.973293, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "96: [D loss: 7.971283, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "97: [D loss: 7.971261, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "98: [D loss: 7.981382, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "99: [D loss: 7.971370, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "100: [D loss: 7.971345, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "101: [D loss: 7.971243, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "102: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "103: [D loss: 7.971521, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "104: [D loss: 7.971397, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "105: [D loss: 7.971362, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "106: [D loss: 7.971197, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "107: [D loss: 7.972029, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "108: [D loss: 7.973027, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "109: [D loss: 7.971223, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "110: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "111: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "112: [D loss: 7.971239, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "113: [D loss: 7.971201, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "114: [D loss: 7.971234, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "115: [D loss: 7.971198, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "116: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "117: [D loss: 7.971370, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "118: [D loss: 7.971200, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "119: [D loss: 7.971365, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "120: [D loss: 7.971245, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "121: [D loss: 7.971254, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "122: [D loss: 7.971203, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "123: [D loss: 7.971200, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "124: [D loss: 7.971224, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "125: [D loss: 7.971336, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "126: [D loss: 7.971251, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "127: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "128: [D loss: 7.971199, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "129: [D loss: 7.971205, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "130: [D loss: 7.972648, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n",
      "131: [D loss: 7.971234, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "132: [D loss: 7.971196, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "133: [D loss: 7.971197, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "134: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "135: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "136: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "137: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "138: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "139: [D loss: 7.971197, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "140: [D loss: 7.971197, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "141: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "142: [D loss: 7.971199, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "143: [D loss: 7.971197, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "144: [D loss: 7.971389, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "145: [D loss: 7.971309, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "146: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "147: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "149: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "150: [D loss: 7.971200, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "151: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "152: [D loss: 7.971300, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "153: [D loss: 7.971195, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "154: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "155: [D loss: 7.971208, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "156: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "157: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "158: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "159: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "160: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "161: [D loss: 7.971196, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "162: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "163: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "164: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "165: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "166: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "167: [D loss: 7.971195, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "168: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "169: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "170: [D loss: 7.971196, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "171: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "172: [D loss: 7.971198, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "173: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "174: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "175: [D loss: 7.971195, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "176: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "177: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "178: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "179: [D loss: 7.971195, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "180: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "181: [D loss: 7.971197, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "182: [D loss: 7.971304, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "183: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "184: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "185: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "186: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "187: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "188: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "189: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "190: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "191: [D loss: 7.971198, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "192: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "193: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "194: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "195: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "196: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "197: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "198: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "199: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "200: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "201: [D loss: 7.971200, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "202: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "203: [D loss: 7.971193, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "204: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "205: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "206: [D loss: 7.971195, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "207: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "208: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "209: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "210: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "211: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "212: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "213: [D loss: 7.971200, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "214: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "215: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "216: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "217: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "218: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "219: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "220: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "221: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "222: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "223: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "224: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "225: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "226: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "227: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "228: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "229: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "230: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "231: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "232: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "233: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "234: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "235: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "236: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "237: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "238: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "239: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "240: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "241: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "242: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "243: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "244: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "245: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "246: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "247: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "248: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "249: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "250: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "251: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "252: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "253: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "254: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "255: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "256: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "257: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "258: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "259: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "260: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "261: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "262: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "263: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "264: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "265: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "266: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "267: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "268: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "269: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "270: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "271: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "272: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "273: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "274: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "275: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "276: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "277: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "278: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "279: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "280: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "281: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "282: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "283: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "284: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "285: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "286: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "287: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "288: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "289: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "290: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "291: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "292: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "293: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "294: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "295: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "296: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "297: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "298: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "299: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "300: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "301: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "302: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "303: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "304: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "305: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "306: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "307: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "308: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "309: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "310: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "311: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "312: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "313: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "314: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "315: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "316: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "317: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "318: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "319: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "320: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "321: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "322: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "323: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "324: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "325: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "326: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "327: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "328: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "329: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "330: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "331: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "332: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "333: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "334: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "335: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "336: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "337: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "338: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "339: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "340: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "341: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "342: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "343: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "344: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "345: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "346: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "347: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "348: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "349: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "350: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "351: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "352: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "353: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "354: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "355: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "356: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "357: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "358: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "359: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "360: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "361: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "362: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "363: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "364: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "365: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "366: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "367: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "369: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "370: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "371: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "372: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "373: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "374: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "375: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "376: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "377: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "378: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "379: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "380: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "381: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "382: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "383: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "384: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "385: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "386: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "387: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "388: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "389: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "390: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "391: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "392: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "393: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "394: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "395: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "396: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "397: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "398: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "399: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "400: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "401: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "402: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "403: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "404: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "405: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "406: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "407: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "408: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "409: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "410: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "411: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "412: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "413: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "414: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "415: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "416: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "417: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "418: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "419: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "420: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "421: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "422: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "423: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "424: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "425: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "426: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "427: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "428: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "429: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "430: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "431: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "432: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "433: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "434: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "435: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "436: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "437: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "438: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "439: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "440: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "441: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "442: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "443: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "444: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "445: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "446: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "447: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "448: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "449: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "450: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "451: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "452: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "453: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "454: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "455: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "456: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "457: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "458: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "459: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "460: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "461: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "462: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "463: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "464: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "465: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "466: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "467: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "468: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "469: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "470: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "471: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "472: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "473: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "474: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "475: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "476: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "477: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "478: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "479: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "480: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "481: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "482: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "483: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "484: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "485: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "486: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "487: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "488: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "489: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "490: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "491: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "492: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "493: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "494: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "495: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "496: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "497: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "498: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "499: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "500: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "501: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "502: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "503: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "504: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "505: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "506: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "507: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "508: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "509: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "510: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "511: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "512: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "513: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "514: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "515: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "516: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "517: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "518: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "519: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "520: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "521: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "522: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "523: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "524: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "525: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "526: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "527: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "528: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "529: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "530: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "531: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "532: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "533: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "534: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "535: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "536: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "537: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "538: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "539: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "540: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "541: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "542: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "543: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "544: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "545: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "546: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "547: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "548: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "549: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "550: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "551: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "552: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "553: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "554: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "555: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "556: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "557: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "558: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "559: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "560: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "561: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "562: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "563: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "564: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "565: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "566: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "567: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "568: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "569: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "570: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "571: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "572: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "573: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "574: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "575: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "576: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "577: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "578: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "579: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "580: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "581: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "582: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "583: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "584: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "585: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "586: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "587: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "589: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "590: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "591: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "592: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "593: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "594: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "595: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "596: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "597: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "598: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "599: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "600: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "601: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "602: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "603: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "604: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "605: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "606: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "607: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "608: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "609: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "610: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "611: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "612: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "613: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "614: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "615: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "616: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "617: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "618: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "619: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "620: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "621: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "622: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "623: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "624: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "625: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "626: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "627: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "628: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "629: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "630: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "631: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "632: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "633: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "634: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "635: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "636: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "637: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "638: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "639: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "640: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "641: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "642: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "643: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "644: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "645: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "646: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "647: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "648: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "649: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "650: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "651: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "652: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "653: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "654: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "655: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "656: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "657: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "658: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "659: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "660: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "661: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "662: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "663: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "664: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "665: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "666: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "667: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "668: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "669: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "670: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "671: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "672: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "673: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "674: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "675: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "676: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "677: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "678: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "679: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "680: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "681: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "682: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "683: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "684: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "685: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "686: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "687: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "688: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "689: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "690: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "691: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "692: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "693: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "694: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "695: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "696: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "697: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "698: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "699: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "700: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "701: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "702: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "703: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "704: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "705: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "706: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "707: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "708: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "709: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "710: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "711: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "712: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "713: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "714: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "715: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "716: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "717: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "718: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "719: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "720: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "721: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "722: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "723: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "724: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "725: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "726: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "727: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "728: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "729: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "730: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "731: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "732: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "733: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "734: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "735: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "736: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "737: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "738: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "739: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "740: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "741: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "742: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "743: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "744: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "745: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "746: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "747: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "748: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "749: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "750: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "751: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "752: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "753: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "754: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "755: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "756: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "757: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "758: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "759: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "760: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "761: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "762: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "763: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "764: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "765: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "766: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "767: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "768: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "769: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "770: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "771: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "772: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "773: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "774: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "775: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "776: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "777: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "778: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "779: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "780: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "781: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "782: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "783: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "784: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "785: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "786: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "787: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "788: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "789: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "790: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "791: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "792: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "793: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "794: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "795: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "796: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "797: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "798: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "799: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "800: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "801: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "802: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "803: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "804: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "805: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "806: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "807: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "809: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "810: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "811: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "812: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "813: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "814: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "815: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "816: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "817: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "818: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "819: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "820: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "821: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "822: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "823: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "824: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "825: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "826: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "827: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "828: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "829: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "830: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "831: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "832: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "833: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "834: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "835: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "836: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "837: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "838: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "839: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "840: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "841: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "842: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "843: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "844: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "845: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "846: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "847: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "848: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "849: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "850: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "851: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "852: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "853: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "854: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "855: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "856: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "857: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "858: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "859: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "860: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "861: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "862: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "863: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "864: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "865: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "866: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "867: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "868: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "869: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "870: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "871: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "872: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "873: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "874: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "875: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "876: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "877: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "878: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "879: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "880: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "881: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "882: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "883: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "884: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "885: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "886: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "887: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "888: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "889: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "890: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "891: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "892: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "893: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "894: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "895: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "896: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "897: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "898: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "899: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "900: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "901: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "902: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "903: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "904: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "905: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "906: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "907: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "908: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "909: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "910: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "911: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "912: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "913: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "914: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "915: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "916: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "917: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "918: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "919: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "920: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "921: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "922: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "923: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "924: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "925: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "926: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "927: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "928: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "929: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "930: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "931: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "932: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "933: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "934: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "935: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "936: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "937: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "938: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "939: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "940: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "941: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "942: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "943: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "944: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "945: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "946: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "947: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "948: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "949: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "950: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "951: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "952: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "953: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "954: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "955: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "956: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "957: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "958: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "959: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "960: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "961: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "962: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "963: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "964: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "965: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "966: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "967: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "968: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "969: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "970: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "971: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "972: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "973: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "974: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "975: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "976: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "977: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "978: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "979: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "980: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "981: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "982: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "983: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "984: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "985: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "986: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "987: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "988: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "989: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "990: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "991: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "992: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "993: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "994: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "995: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "996: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "997: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "998: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "999: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1000: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1001: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1002: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1003: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1004: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1005: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1006: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1007: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1008: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1009: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1010: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1011: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1012: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1013: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1014: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1015: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1016: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1017: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1018: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1019: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1020: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1021: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1022: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1023: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1024: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1025: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1026: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1027: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1029: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1030: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1031: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1032: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1033: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1034: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1035: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1036: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1037: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1038: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1039: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1040: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1041: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1042: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1043: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1044: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1045: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1046: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1047: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1048: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1049: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1050: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1051: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1052: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1053: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1054: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1055: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1056: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1057: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1058: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1059: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1060: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1061: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1062: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1063: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1064: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1065: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1066: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1067: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1068: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1069: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1070: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1071: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1072: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1073: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1074: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1075: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1076: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1077: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1078: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1079: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1080: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1081: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1082: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1083: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1084: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1085: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1086: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1087: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1088: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1089: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1090: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1091: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1092: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1093: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1094: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1095: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1096: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1097: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1098: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1099: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1100: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1101: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1102: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1103: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1104: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1105: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1106: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1107: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1108: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1109: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1110: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1111: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1112: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1113: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1114: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1115: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1116: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1117: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1118: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1119: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1120: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1121: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1122: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1123: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1124: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1125: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1126: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1127: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1128: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1129: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1130: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1131: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1132: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1133: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1134: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1135: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1136: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1137: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1138: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1139: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1140: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1141: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1142: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1143: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1144: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1145: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1146: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1147: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1148: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1149: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1150: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1151: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1152: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1153: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1154: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1155: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1156: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1157: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1158: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1159: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1160: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1161: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1162: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1163: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1164: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1165: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1166: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1167: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1168: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1169: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1170: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1171: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1172: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1173: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1174: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1175: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1176: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1177: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1178: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1179: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1180: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1181: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1182: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1183: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1184: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1185: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1186: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1187: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1188: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1189: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1190: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1191: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1192: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1193: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1194: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1195: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1196: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1197: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1198: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1199: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1200: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1201: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1202: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1203: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1204: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1205: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1206: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1207: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1208: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1209: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1210: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1211: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1212: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1213: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1214: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1215: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1216: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1217: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1218: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1219: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1220: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1221: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1222: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1223: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1224: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1225: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1226: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1227: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1228: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1229: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1230: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1231: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1232: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1233: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1234: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1235: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1236: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1237: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1238: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1239: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1240: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1241: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1242: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1243: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1244: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1245: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1246: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1247: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1248: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1249: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1250: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1251: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1252: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1253: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1254: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1255: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1256: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1257: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1258: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1259: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1260: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1261: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1262: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1263: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1264: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1265: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1266: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1267: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1268: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1269: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1270: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1271: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1272: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1273: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1274: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1275: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1276: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1277: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1278: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1279: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1280: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1281: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1282: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1283: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1284: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1285: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1286: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1287: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1288: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1289: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1290: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1291: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1292: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1293: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1294: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1295: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1296: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1297: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1298: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1299: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1300: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1301: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1302: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1303: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1304: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1305: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1306: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "1307: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DCGAN on MNIST using Keras\n",
    "Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "Dependencies: tensorflow 1.0 and keras 2.0\n",
    "Usage: python3 dcgan_mnist.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "\n",
    "    # (WF+2P)/S+1\n",
    "    def discriminator(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.4\n",
    "        # In: 28 x 28 x 1, depth = 1\n",
    "        # Out: 14 x 14 x 1, depth=64\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        # Out: 1-dim probability\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        dropout = 0.4\n",
    "        depth = 64+64+64+64\n",
    "        dim = 7\n",
    "        # In: 100\n",
    "        # Out: dim x dim x depth\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "\n",
    "        # In: dim x dim x depth\n",
    "        # Out: 2*dim x 2*dim x depth/2\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.AM\n",
    "\n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        self.x_train = input_data.read_data_sets(\"mnist\",\\\n",
    "            one_hot=True).train.images\n",
    "        self.x_train = self.x_train.reshape(-1, self.img_rows,\\\n",
    "            self.img_cols, 1).astype(np.float32)\n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,\n",
    "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
    "                        noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mnist_dcgan = MNIST_DCGAN()\n",
    "    timer = ElapsedTimer()\n",
    "    mnist_dcgan.train(train_steps=10000, batch_size=256, save_interval=500)\n",
    "    timer.elapsed_time()\n",
    "    mnist_dcgan.plot_images(fake=True)\n",
    "    mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Additional MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "# set up your data generator\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip = False)\n",
    "\n",
    "# Fit the generator using your data\n",
    "datagen.fit(X_train.reshape((len(X_train), 28, 28, 1)))\n",
    "\n",
    "image = X_train[5]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(4, 4, 1)\n",
    "plt.imshow(image.reshape((28,28)),  cmap='gray')\n",
    "\n",
    "for j in range(15):\n",
    "    augmented = datagen.random_transform(image.reshape((28,28,1)))\n",
    "    plt.subplot(4, 4, j+2)\n",
    "    plt.imshow(augmented.reshape((28,28)),  cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
